{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#all_slow\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "IN_MAIN = __name__ == '__main__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infrastructure for copying notebooks\n",
    "if IN_COLAB and IN_MAIN:\n",
    "    home_dir = '/content/drive/MyDrive/Colab Notebooks/Ecosystems/v3'\n",
    "if IN_COLAB and IN_MAIN:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import sys\n",
    "    sys.path.append(home_dir)\n",
    "    %cd $home_dir\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB and IN_MAIN:\n",
    "    !cd;pip -q install import-ipynb\n",
    "    !cd;pip -q install stable-baselines3[extra]\n",
    "    !cd;apt install swig\n",
    "    !cd;pip -q install box2d box2d-kengz\n",
    "    #verbose = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "from IPython.display import HTML\n",
    "import gym\n",
    "from gym import spaces\n",
    "# This has to be imported before our own notebook imports.\n",
    "#import import_ipynb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from ecotwins.animal_classes import Ecosystem, MultiSheep, SimpleSheep, Terrain\n",
    "# import ecoenv\n",
    "from ecotwins.ecoenv import EcoEnv\n",
    "# from perception import Perception\n",
    "from stable_baselines3 import PPO, A2C, SAC, DDPG, TD3 # , DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# from utility import distance, draw_objects, motion_diagram, normalize\n",
    "from ecotwins.animation_helper import AnimationHelper\n",
    "from ecotwins.reflex_agent import ReflexAgent\n",
    "from ecotwins.animal_classes import Ecosystem, MultiSheep, SimpleSheep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecosystem experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World 2 (One need)\n",
    "* Food objects that respawn when consumed\n",
    "* Automatic consumption when close\n",
    "* Reward = +1 per food item consumed\n",
    "* Input: food direction\n",
    "* Output: steering direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Terrain(objects={'dandelion': 1})\n",
    "# t = Terrain(objects={'dandelion': (np.random.random((100,2)) - 0.5) * 20})\n",
    "hyperparameters = {'max_age': 2000, 'delta': 0.2, 'close': 5, 'gamma': 0.9}\n",
    "agent = SimpleSheep(distances={'dandelion':28}, hyperparameters=hyperparameters) \n",
    "eco = Ecosystem(t, agent)\n",
    "env = EcoEnv(eco)\n",
    "\n",
    "# Create the model\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "# model.set_env(env)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=2e5)\n",
    "\n",
    "# Evaluate the model\n",
    "# NOTE: If you use wrappers with your environment that modify rewards,\n",
    "#       this will be reflected here. To evaluate with original rewards,\n",
    "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f'Mean reward: {mean_reward}, std. dev.: {std_reward}')\n",
    "\n",
    "# Save the model\n",
    "#model.save(\"ecosystem\")\n",
    "#del model  # delete trained model to demonstrate loading\n",
    "\n",
    "# Load the trained model\n",
    "#model = DQN.load(\"ecosystem\", env=env)\n",
    "\n",
    "# Enjoy trained model\n",
    "# obs = env.reset() # Generate a new map? Returns an initial observation\n",
    "# trace = [env.position.copy()]\n",
    "# total_reward = 0\n",
    "# # for i in range(ecoenv.TRACE_LENGTH): # Take a walk of length ecoenv.TRACE_LENGTH (not EPISODE_LENGTH as in training)\n",
    "# for i in range(2000):\n",
    "#     action, _states = model.predict(obs, deterministic=True) # Select action\n",
    "#     obs, reward, dones, info = env.step(action) # Compute consequences\n",
    "#     assert(reward >= 0)\n",
    "#     total_reward += reward\n",
    "#     trace.append(env.position.copy())\n",
    "# trace = np.array(trace)\n",
    "# env.render(trace) # Show walk\n",
    "# plt.title(f'Total reward: {total_reward}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "a = AnimationHelper(env, model)\n",
    "a.init_animation();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(a.fig, a.drawframe, frames=2000, interval=50, blit=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World 4 (Two needs)\n",
    "* Food and water objects that disappear and respawn\n",
    "* Automatic consumption when close\n",
    "* reward = energy*water\n",
    "* Input: energy direction and water direction\n",
    "* Output: steering direction.\n",
    "* Movement costs enery and water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def world4():\n",
    "    terrain_args = {\"objects\": {\"dandelion\": 20, \"water\": 20}}\n",
    "    # terrain_args = {\"objects\": {\"dandelion\": 20*(np.random.random((20,2))-.5),\n",
    "                                # \"water\": 20*(np.random.random((20,2))-.5)}}\n",
    "    agent_args = {\n",
    "        \"hyperparameters\": {\"max_age\": 2000, \"delta\": 0.1, \"close\": 0.5},\n",
    "        # \"distances\": {\"dandelion\": 28, \"water\": 28},\n",
    "        \"distances\": {\"dandelion\": 5, \"water\": 5},\n",
    "        \"interoception\": {\"energy\": 3, \"water\": 3},\n",
    "        \"use_interoception_as_obs\": True,\n",
    "        \"use_intensity_as_obs\": True,\n",
    "        \"use_multi_direction\": True,\n",
    "        \"use_single_direction\": True,\n",
    "        \"use_happiness_as_obs\": True, \n",
    "        \"use_reward_radius_in_perception\": True,\n",
    "        \"normalize_action\": False,\n",
    "        # \"action_noise\": 0.05,\n",
    "        \"homeostatic_effects\": {  # Nutrition table\n",
    "            (\"move\", None): {\"energy\": -0.01, \"water\": -0.01},\n",
    "            (\"consume\", \"water\"): {\"energy\": 0, \"water\": 1},\n",
    "            (\"consume\", \"dandelion\"): {\"energy\": 1, \"water\": 0.0},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    t = Terrain(**terrain_args)\n",
    "    agent = MultiSheep(**agent_args)\n",
    "    eco = Ecosystem(t, agent)\n",
    "    env = EcoEnv(eco)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from ecotwins.utility import normalize\n",
    "\n",
    "class RandomAgent:\n",
    "    def __init__(self, eco, n_dir=2):\n",
    "        self.eco = eco\n",
    "        self.cur_dir = 0\n",
    "        self.max_dir = n_dir \n",
    "\n",
    "         # Used to determine when to change direction. The direction is changed when\n",
    "         # we have found/consemd an object.\n",
    "        self.e_levels = env.agent.interoception.copy()\n",
    "\n",
    "\n",
    "    def predict(self, observation, **kwargs):\n",
    "        action = normalize(np.random.random((2)) - 0.5)\n",
    "        return action, None # None needed in order to mimic stable-baseline3\n",
    "\n",
    "\n",
    "    def episode(self, n_step=None):\n",
    "        n_steps = np.iinfo(int).max if n_step is None else n_steps\n",
    "        obs = self.eco.reset()\n",
    "        \n",
    "        for i in range(n_steps):\n",
    "            action = normalize(np.random.random((2)) - 0.5)\n",
    "            obs, reward, done, _ = self.eco.step(action)\n",
    "\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "env = world4()\n",
    "env.agent.hyperparameters['delta'] = 1\n",
    "model = RandomAgent(env)\n",
    "a = AnimationHelper(env, model)\n",
    "a.init_animation();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(a.fig, a.drawframe, frames=200, interval=50, blit=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflex agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "env = world4()\n",
    "model = ReflexAgent(env, n_dir=4)\n",
    "a = AnimationHelper(env, model)\n",
    "a.init_animation();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(a.fig, a.drawframe, frames=200, interval=50, blit=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extend_training = False\n",
    "env = world4()\n",
    "num_cpu = 4\n",
    "env = DummyVecEnv([world4 for i in range(num_cpu)])\n",
    "\n",
    "device = 'cpu'\n",
    "if not extend_training:\n",
    "    # Instantiate the agent\n",
    "    model = A2C('MlpPolicy', env, verbose=1, use_sde=True, device=device)\n",
    "    # model = TD3('MlpPolicy', env, verbose=1, device='cuda')\n",
    "else:\n",
    "    model.set_env(env)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=2e5, log_interval=400)\n",
    "# model.learn(total_timesteps=2e5)\n",
    "\n",
    "# Evaluate the model\n",
    "mean_reward, std_reward = evaluate_policy(model, world4(), n_eval_episodes=10)\n",
    "print(f'Mean reward: {mean_reward:.2f}, std. dev.: {std_reward:.2f}')\n",
    "\n",
    "# Save the model\n",
    "# model.save(\"multi_sheep_champion.pth\")\n",
    "\n",
    "# Load the model\n",
    "# model = PPO.load(\"multi_sheep_champion.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "env = world4()\n",
    "a = AnimationHelper(env, model)\n",
    "a.init_animation(show_consumed=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(a.fig, a.drawframe, frames=2000, interval=50, blit=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"multi_sheep_wo_normalize-2021-12-13.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFwriter = animation.FFMpegWriter(fps=60)\n",
    "# anim.save('animation.mp4', writer = FFwriter)\n",
    "# anim = animation.FuncAnimation(a.fig, a.drawframe, frames=200, interval=50, blit=True)\n",
    "\n",
    "anim.save('multi_sheep_20211213-with-happiness-obs.mp4',\n",
    "          fps=30,\n",
    "          extra_args=['-vcodec', 'h264',  '-pix_fmt', 'yuv420p']\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World 6 (Three needs) Kanske skakar ett litet nätverk mindre?\n",
    "* Three kinds of respawning consumable objects: dandelion, water, grass\n",
    "* Automatic consumption when close\n",
    "* reward = energy * water * protein\n",
    "* Input: 3 x object perception\n",
    "* Output: steering direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def world6():\n",
    "    terrain_args = {\n",
    "        # \"objects\": {\"dandelion\": 100, \"water\": 100, \"grass\": 100},\n",
    "        \"objects\": {\"dandelion\": 20, \"water\": 20, \"grass\": 20},\n",
    "        \"torus\": False\n",
    "    }\n",
    "    #     terrain_args = {\"objects\": {\"dandelion\": 200,   \"water\": 200,   \"grass\": 200}}\n",
    "    # terrain_args = {\"objects\": {\"dandelion\": 20*(np.random.random((20,2))-.5), \"water\": 20*(np.random.random((20,2))-.5), \"grass\": 20*(np.random.random((20,2))-.5)}}\n",
    "    agent_args = {\n",
    "        \"hyperparameters\": {\"max_age\": 2000, \"delta\": 0.1, \"close\": .5, \"gamma\": 0.9},\n",
    "        # \"distances\": {\"dandelion\": 28, \"water\": 28},\n",
    "        \"distances\": {\"dandelion\": 5, \"water\": 5, \"grass\": 5},\n",
    "        \"interoception\": {\"energy\": 3, \"water\": 3, \"protein\": 3},\n",
    "        \"use_interoception_as_obs\": True,\n",
    "        \"use_intensity_as_obs\": True,\n",
    "        \"use_single_direction\": True,\n",
    "        \"use_multi_direction\": True,\n",
    "        \"use_happiness_as_obs\": True,\n",
    "        \"use_reward_radius_in_perception\": True,\n",
    "        \"normalize_action\": False,\n",
    "#         \"perception_noise\": 0.01,\n",
    "        \"homeostatic_effects\": {  # Nutrition table\n",
    "            (\"move\", None): {\"energy\": -0.01, \"water\": -0.01, \"protein\": -0.01},\n",
    "            (\"consume\", \"water\"): {\"energy\": 0, \"water\": 1, \"protein\": 0},\n",
    "            (\"consume\", \"dandelion\"): {\"energy\": 1, \"water\": 0, \"protein\": 0},\n",
    "            (\"consume\", \"grass\"): {\"energy\": 0, \"water\": 0, \"protein\": 1},\n",
    "        },\n",
    "        \"n_frames\":1,\n",
    "    }\n",
    "\n",
    "    t = Terrain(**terrain_args)\n",
    "    agent = MultiSheep(**agent_args)\n",
    "    eco = Ecosystem(t, agent)\n",
    "    env = EcoEnv(eco)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extend_training = True\n",
    "env = world6()\n",
    "\n",
    "if not extend_training:\n",
    "    # Instantiate the agent\n",
    "    device='cpu'\n",
    "    model = A2C('MlpPolicy', env, verbose=1, use_sde=True, device=device)\n",
    "else:\n",
    "    model.set_env(env)\n",
    "\n",
    "# model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=2e5, log_interval=400)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f'Mean reward: {mean_reward}, std. dev.: {std_reward}')\n",
    "\n",
    "# Save the model\n",
    "# model.save(\"multi_sheep_with_normalization.pth\")\n",
    "\n",
    "# Load the model\n",
    "# model = PPO.load(\"multi_sheep_with_normalization.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "env = world6()\n",
    "a = AnimationHelper(env, model)\n",
    "a.init_animation(show_consumed=False);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(a.fig, a.drawframe, frames=2000, interval=50, blit=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFwriter = animation.FFMpegWriter(fps=60)\n",
    "# anim.save('3needs.mp4', writer = FFwriter)\n",
    "anim.save('3needs.mp4',\n",
    "          fps=30,\n",
    "          extra_args=['-vcodec', 'h264',  '-pix_fmt', 'yuv420p']\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflex agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "env = world6()\n",
    "m = ReflexAgent(env, 3)\n",
    "a = AnimationHelper(env, m)\n",
    "a.init_animation(show_consumed=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(a.fig, a.drawframe, frames=200, interval=50, blit=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thorny world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thorny world. Food and thorn objects. Lose energy (blood) when close to thorn. \n",
    "\n",
    "def thorny_world():\n",
    "    terrain_args = {\"objects\": {\"dandelion\": 20, \"thorn\": 20}}\n",
    "    # terrain_args = {\"objects\": {\"dandelion\": 20*(np.random.random((20,2))-.5),\n",
    "                                # \"water\": 20*(np.random.random((20,2))-.5)}}\n",
    "    agent_args = {\n",
    "        \"hyperparameters\": {\"max_age\": 2000, \"delta\": 0.1, \"close\": .25},\n",
    "        # \"distances\": {\"dandelion\": 28, \"water\": 28},\n",
    "        \"distances\": {\"dandelion\": 10, \"thorn\": 5},\n",
    "        \"interoception\": {\"energy\": 3},\n",
    "        \"use_interoception_as_obs\": True,\n",
    "        \"use_intensity_as_obs\": True,\n",
    "        \"use_single_direction\": True,\n",
    "        \"use_multi_direction\": True,\n",
    "        \"use_reward_radius_in_perception\": True,\n",
    "        \"use_happiness_as_obs\": True,\n",
    "        \"normalize_action\": False,\n",
    "        \"homeostatic_effects\": {  # Nutrition table\n",
    "            (\"move\", None): {\"energy\": -0.01},\n",
    "            (\"consume\", \"thorn\"): {\"energy\": -5},\n",
    "            (\"consume\", \"dandelion\"): {\"energy\": 1},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    t = Terrain(**terrain_args)\n",
    "    agent = MultiSheep(**agent_args) # need: only energy, object perception: dandelion and thorn\n",
    "    eco = Ecosystem(t, agent)\n",
    "    env = EcoEnv(eco)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = thorny_world()\n",
    "\n",
    "# Create the model\n",
    "# model = PPO('MlpPolicy', env, verbose=1, use_sde=True, tensorboard_log='./tb_logs')\n",
    "model = A2C('MlpPolicy', env, verbose=1, use_sde=True, tensorboard_log='./tb_logs', device='cuda')\n",
    "# model = A2C('MlpPolicy', env, verbose=1, use_sde=True, tensorboard_log='./tb_logs', device='cpu')\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=2e5, log_interval=400, eval_freq=10000)\n",
    "\n",
    "# Evaluate the model\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f'Mean reward: {mean_reward:.2f}, std. dev.: {std_reward:.2f}')\n",
    "\n",
    "# Save the model\n",
    "# model.save(\"multi_sheep_champion.pth\")\n",
    "\n",
    "# Load the model\n",
    "# model = PPO.load(\"multi_sheep_champion.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Perhaps a somewhat unclean way of suppressing the cell output.\n",
    "env = thorny_world()\n",
    "a = AnimationHelper(env, model)\n",
    "a.init_animation(show_consumed=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(a.fig, a.drawframe, frames=2000, interval=50, blit=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL agent with reward similar to World 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "# The modified happiness function\n",
    "def h(agent, t=None):\n",
    "    return agent.interoception['energy'] + agent.p_happiness\n",
    "\n",
    "\n",
    "env = thorny_world()\n",
    "env.agent._init_p_happiness = types.MethodType(lambda x: 0.0, env.agent)\n",
    "env.agent.happiness = types.MethodType(h, env.agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C('MlpPolicy', env, verbose=1, use_sde=True, tensorboard_log='./tb_logs', device='cpu')\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=300, log_interval=400, eval_freq=10000)\n",
    "\n",
    "# Evaluate the model\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f'Mean reward: {mean_reward:.2f}, std. dev.: {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations\n",
    "* Vary object quantity\n",
    "* Vary object nutrition\n",
    "* Make the objects move\n",
    "* reward as happiness or happiness delta\n",
    "* Vary stimulus function\n",
    "* Vary perception radius and reward radius\n",
    "* With or without weber \n",
    "* Nearest direction or multi-direction\n",
    "✈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "*   Death criteria (alive): Should be formulated in terms of homeostasis, not in terms of interoception or happiness = 0. Define alive as a function in the class Organism. Cf. the function in Dandelion.\n",
    "* Add support for simple (asexual) reproduction. Respawning is a form of reproduction for plants and animals (when dying). Plants can be added or removed in other ways, e.g. at random times, not just when grazed. Not water pools though. \n",
    "* Distances should depend on object type (and agent). Both perception radius (elephant vs mosquito) and reward radius.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible next steps\n",
    "\n",
    "**More sheep worlds**\n",
    "\n",
    "* One-need sheep (energy). Food takes time to eat. Eat during 10 steps. Small reward when close. Then repawn. Use multi (partially eaten food).\n",
    "* Two-need sheep (energy and water). Objects: food, water, thorns. \n",
    "* Modest sheep (energy and water).\n",
    "    * Max energy and water levels (max_homeostasis). \n",
    "    * Add consciousness: energy, water (interoception) + happiness\n",
    "\n",
    "**New animals**\n",
    "\n",
    "More details in animal_classes.\n",
    "\n",
    "* Moth + its world with lightbulb objects. Modify/replace world 2.\n",
    "* Silverfish + its world. Same as the moth's world.\n",
    "* Pill bug + its world. \n",
    "    * Energy consumption is proportional to speed^2\n",
    "    * Good to increase speed when in a dry place.\n",
    "* Cat + its world.\n",
    "    * Energy consumption is proportional to speed^2\n",
    "    * There are mice objects too. They move randomly.\n",
    "    * Good to increase speed when gradient_intensity is high (near mice) \n",
    "* Social sheep + its world \n",
    "    * There are sheep objects too. They move randomly or not at all.\n",
    "    * Add oxytocin when near sheep objects. Add number of sheep near (or total_intensity)\n",
    "    * Burn oxytocin with time: -0.01\n",
    "* Discrete sheep + its world\n",
    "    * Action space: up, down, left, right. \n",
    "    * Also good for Pacman.\n",
    "* Fish + its world.\n",
    "    * Use a 3D observation and action spaces. \n",
    "    * Also good for flying animals\n",
    "* Try to find the perfect nursery for learning to survive as fast as posible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animation helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export animation_helper\n",
    "class AnimationHelper:\n",
    "    def __init__(self, env, model):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "\n",
    "    def init_animation(self, show_consumed=True):\n",
    "\n",
    "        env = self.env\n",
    "        self.show_consumed = show_consumed\n",
    "\n",
    "        # create a figure and axes\n",
    "        # self.fig = plt.figure(figsize=(12, 12))\n",
    "        # self.ax1 = plt.subplot(1, 1, 1)\n",
    "        # self.ax1 = plt.axes()\n",
    "        self.fig, self.ax1 = plt.subplots()\n",
    "        self.fig.set_size_inches(12,12)\n",
    "\n",
    "        # set up the subplots as needed\n",
    "        s = env.ecosystem.terrain.space\n",
    "        lower_left = [s[0, 0], s[1, 0]]\n",
    "        x_side = s[0, 1] - s[0, 0]\n",
    "        y_side = s[1, 1] - s[1, 0]\n",
    "        x_lim, y_lim = s[0], s[1]\n",
    "\n",
    "        self.ax1.set_xlim(x_lim)\n",
    "        self.ax1.set_ylim(y_lim)\n",
    "\n",
    "        self.obs = self.env.reset()\n",
    "\n",
    "        # TODO: Add all circles as is done with objects below.\n",
    "        self.circle = matplotlib.patches.Circle(\n",
    "            env.position, next(iter(env.agent.distances.values())), fc=\"y\", alpha=0.1\n",
    "        )\n",
    "        self.reward_radius = env.agent.hyperparameters[\"close\"]\n",
    "        self.reward_circle = matplotlib.patches.Circle(\n",
    "            env.position, self.reward_radius, fc=\"r\", alpha=0.3\n",
    "        )\n",
    "        self.ax1.add_patch(self.circle)\n",
    "        self.ax1.add_patch(self.reward_circle)\n",
    "\n",
    "        # This region might not be needed if we only plotting region is the terrain\n",
    "        # space.\n",
    "        env_region = matplotlib.patches.Rectangle(\n",
    "            lower_left, x_side, y_side, fc=\"b\", alpha=0.05\n",
    "        )\n",
    "        self.ax1.add_patch(env_region)\n",
    "\n",
    "        # objects_plot = ax1.scatter(objects[:,0], objects[:,1], marker='x')\n",
    "        self.objects_plot = {}\n",
    "        for name, pts in env.ecosystem.terrain.objects.items():\n",
    "            self.objects_plot[name] = self.ax1.scatter(pts[:, 0], pts[:, 1], label=name)\n",
    "        self.consumed_plot = {\n",
    "            name: self.ax1.scatter([], [], marker=\"x\", label=f\"Consumed {name}\")\n",
    "            for name in env.ecosystem.terrain.objects.keys()\n",
    "        } if self.show_consumed else {}\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        # create objects that will change in the animation. These are\n",
    "        # initially empty, and will be given new values for each frame\n",
    "        # in the animation.\n",
    "        self.txt_title = self.ax1.set_title(\"Initial configuration\")\n",
    "        (self.line1,) = self.ax1.plot(\n",
    "            [], [], \"g\", lw=2\n",
    "        )  # ax.plot returns a list of 2D line objects\n",
    "        (self.line2,) = self.ax1.plot([], [], \"r\", lw=2)\n",
    "        self.trace = []\n",
    "\n",
    "        # obs_action stores observation, action tuples.\n",
    "        self.obs_action = []\n",
    "\n",
    "        return (\n",
    "            self.line1,\n",
    "            *self.objects_plot.values(),\n",
    "            *self.consumed_plot.values(),\n",
    "            self.circle,\n",
    "            self.reward_circle,\n",
    "        )\n",
    "\n",
    "    ### drawframe cell starts here\n",
    "    # animation function. This is called sequentially\n",
    "    def _gen_title_string(self, n):\n",
    "        title = [f'Frame: {n:4d}']\n",
    "        for name, v in self.env.agent.interoception.items():\n",
    "            title.append(f'{name}: {v:.2f}')\n",
    "\n",
    "        terrain = self.env.ecosystem.terrain\n",
    "        title.append(f'Happiness: {self.env.agent.happiness(terrain):.2f}')\n",
    "        return ' '.join(title)\n",
    "\n",
    "    def drawframe(self, n):\n",
    "        obs = self.obs\n",
    "        model = self.model\n",
    "        env = self.env\n",
    "\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        self.obs_action.append((obs, action))\n",
    "        self.obs, reward, dones, info = env.step(action)\n",
    "        self.trace.append(env.position.copy())\n",
    "        x, y = zip(*self.trace)\n",
    "        self.line1.set_data(x, y)\n",
    "        for name, ax_obj in self.objects_plot.items():\n",
    "            ax_obj.set_offsets(env.ecosystem.terrain.objects[name])\n",
    "\n",
    "        if self.show_consumed:\n",
    "            for name, ax_obj in self.consumed_plot.items():\n",
    "                ax_obj.set_offsets(\n",
    "                    np.array(env.ecosystem.terrain.consumed[name]).reshape(-1, 2)\n",
    "                )\n",
    "\n",
    "        self.circle.center = env.position\n",
    "        self.reward_circle.center = env.position\n",
    "\n",
    "        # title = f'Frame = {n:4d}, '\\\n",
    "        #     f'Energy: {env.agent.interoception[\"energy\"]:.2f}, '\\\n",
    "        #     f'Water: {env.agent.interoception[\"water\"]:.2f}, '\\\n",
    "        #     f'Protein: {env.agent.interoception[\"protein\"]:.2f}, '\\\n",
    "        #     f'Happiness: {env.agent.happiness():.2f}'\n",
    "        # self.txt_title.set_text(title)\n",
    "        self.txt_title.set_text(self._gen_title_string(n))\n",
    "\n",
    "        if dones:\n",
    "            print(f'Done @{n}, {env.agent.age}, {env.ecosystem.terrain.counter}')\n",
    "            self.trace.clear()\n",
    "            env.reset()\n",
    "        return (\n",
    "            self.line1,\n",
    "            *self.objects_plot.values(),\n",
    "            *self.consumed_plot.values(),\n",
    "            self.circle,\n",
    "            self.reward_circle,\n",
    "        )\n",
    "\n",
    "\n",
    "if False and __name__ == \"__main__\":\n",
    "    env = MothEnv()\n",
    "    a = AnimationHelper(env, 4)\n",
    "    print(\"Some elementary test should be here\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReflexAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
