{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp ecoenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "IN_MAIN = __name__ == '__main__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infrastructure for copying notebooks\n",
    "if IN_COLAB and IN_MAIN:\n",
    "    home_dir = '/content/drive/MyDrive/Colab Notebooks/Ecosystems/v3'\n",
    "if IN_COLAB and IN_MAIN:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import sys\n",
    "    sys.path.append(home_dir)\n",
    "    %cd $home_dir\n",
    "    \n",
    "!pip -q install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utility.ipynb\n",
      "importing Jupyter notebook from animal_classes.ipynb\n",
      "importing Jupyter notebook from perception.ipynb\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "# import import_ipynb\n",
    "from ecotwins.utility import distance, motion_diagram \n",
    "from ecotwins.animal_classes import Ecosystem, Terrain, Animal, SimpleSheep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we (re)move this cell?\n",
    "\n",
    "ORIGIN = np.array([0.,0.])\n",
    "SIDE = 20\n",
    "N_OBJECTS = 10\n",
    "\n",
    "# Agent settings\n",
    "RADIUS = SIDE/2\n",
    "DELTA = 0.01 #1e-2\n",
    "REWARD_RADIUS = SIDE/10\n",
    "# TRACE_LENGTH = 1000\n",
    "\n",
    "# Training settings\n",
    "EPISODE_LENGTH = 2000\n",
    "# EPISODE_LENGTH = np.int(np.round(SIDE/DELTA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we (re)move this cell?\n",
    "\n",
    "def generate_objects(side=None, n_objects=N_OBJECTS):\n",
    "    \"\"\"Generates a random map of objects \n",
    "    param side: size of each side of the grid\n",
    "    param n_objects: (initial) number of objects in the ecosystem\n",
    "    \"\"\"\n",
    "    objects=(np.random.rand(n_objects,2)-0.5)*side # Takes values in (-ecoenv.SIDE/2,+ecoenv.SIDE/2)\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The EcoEnv class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EcoEnv(gym.Env): \n",
    "    \"\"\"An ecosystem environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']} #Ta ej bort\n",
    "\n",
    "    #A functon that takes an ecosystem and returns a gym\n",
    "    def __init__(self, ecosystem):\n",
    "        super(EcoEnv, self).__init__()\n",
    "        \n",
    "        self.ecosystem = ecosystem       \n",
    "        self.agent = ecosystem.agent\n",
    "        self.perception = ecosystem.agent.perception\n",
    "        self.position = self.agent.position.copy()\n",
    "        self.p_happiness = self.agent.happiness(self.ecosystem.terrain)\n",
    "        self.current_step = 0\n",
    "      \n",
    "        self.action_space = self.agent.action_space\n",
    "        self.observation_space = self.agent.observation_space\n",
    "        # debug\n",
    "        self.total_reward = 0\n",
    "        # An attempt to reward stayin alive\n",
    "        self.age_reward = (self.agent.happiness(self.ecosystem.terrain) /\n",
    "                           self.agent.hyperparameters['max_age']\n",
    "                          )\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return self.agent.observation(self.ecosystem.terrain)\n",
    "\n",
    "    # Helper function to step.\n",
    "    def _take_action(self, action): \n",
    "        # Maybe this should be handled by the ecosystem ie the call should be\n",
    "        # self.ecosystem.update(action, agent)\n",
    "#         print(type(self))\n",
    "        self.agent.update(action, self.ecosystem.terrain)\n",
    "#     terrain update as well\n",
    "        self.position = self.agent.position.copy()\n",
    "        return self.position\n",
    "\n",
    "    # Reward functions\n",
    "    def _reward(self): \n",
    "        happiness = self.agent.happiness(self.ecosystem.terrain)\n",
    "        r = happiness - self.p_happiness\n",
    "        self.p_happiness = happiness\n",
    "        # TODO: Clean handling of resource punishment\n",
    "        return r + self.age_reward - 3 * self.agent.out_of_resources() \n",
    "\n",
    "    def _is_done(self):\n",
    "        \"\"\"Done if maximum number of step exceed or if\n",
    "        we are outside the region. \"\"\" # or if we die?\n",
    "        return self.current_step > EPISODE_LENGTH or self._is_outside()\n",
    "    \n",
    "    def _is_outside(self): return np.abs(self.position).max() > self.side / 2\n",
    "\n",
    "    def _is_close(self):\n",
    "        return distance(self.position, self.objects).min() < self.reward_radius\n",
    "\n",
    "    # Execute one time step within the environment\n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        self.current_step += 1\n",
    "        \n",
    "        reward = self._reward()\n",
    "        self.total_reward += reward\n",
    "        obs = self._next_observation()\n",
    "        done = self.ecosystem.is_done()\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    # Reset the state of the environment to an initial state\n",
    "    def reset(self):\n",
    "        print(f\"Reset@{self.current_step}, accumulated reward: {self.total_reward:.2f}\", end=\"\")\n",
    "        print(\", Interoception levels: \", end=\"\")\n",
    "        print(*[f'{k}:{v:.2f}' for k,v in self.agent.interoception.items()], sep=', ', end=\"\")\n",
    "        print(f' happiness: {self.agent.happiness(self.ecosystem.terrain):.2f}')\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        self.ecosystem.reset()\n",
    "        self.position = self.agent.position.copy()\n",
    "        self.current_step = 0\n",
    "        self.p_happiness = self.agent.happiness(self.ecosystem.terrain)\n",
    "#         self.consumed = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    # Render the environment to the screen\n",
    "    def render(self, trace, mode='human', close=False):\n",
    "        # @TODO This should be handled properly\n",
    "        side = self.ecosystem.terrain.space[0,1] - self.ecosystem.terrain.space[0,0]\n",
    "        # @TODO Should support any number of object types not just one.\n",
    "        objects = next(iter(self.ecosystem.terrain.objects.values()))\n",
    "        motion_diagram(objects, trace, side)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_MAIN:\n",
    "#     t = Terrain(objects={'dandelion':(np.random.random((10,2)) - 0.5) * 20})\n",
    "    t = Terrain(objects={'dandelion': 10})\n",
    "    hyperparameters = {'max_age': 2000, 'delta': 0.1, 'close': 1, 'gamma': 0.9}\n",
    "    agent = SimpleSheep(distances={'dandelion':10}) \n",
    "    eco = Ecosystem(t, agent)\n",
    "    env = EcoEnv(eco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted animal_classes.ipynb.\n",
      "Converted animation_helper.ipynb.\n",
      "Converted ecoenv.ipynb.\n",
      "Converted happiness.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted perception.ipynb.\n",
      "Converted reflex_agent.ipynb.\n",
      "Converted tyckande.ipynb.\n",
      "Converted utility.ipynb.\n",
      "Converted worlds.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
