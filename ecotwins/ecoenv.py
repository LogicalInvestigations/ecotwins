# AUTOGENERATED! DO NOT EDIT! File to edit: ecoenv.ipynb (unless otherwise specified).

__all__ = ['EcoEnv']

# Cell
import gym
import numpy as np
import matplotlib.pyplot as plt
from gym import spaces
# import import_ipynb
from .utility import distance, motion_diagram
from .animal_classes import Ecosystem, Terrain, Animal, SimpleSheep

# Cell
class EcoEnv(gym.Env):
    """An ecosystem environment for OpenAI gym"""
    metadata = {'render.modes': ['human']} #Ta ej bort

    #A functon that takes an ecosystem and returns a gym
    def __init__(self, ecosystem):
        super(EcoEnv, self).__init__()

        self.ecosystem = ecosystem
        self.agent = ecosystem.agent
        self.perception = ecosystem.agent.perception
        self.position = self.agent.position.copy()
        self.p_happiness = self.agent.happiness(self.ecosystem.terrain)
        self.current_step = 0

        self.action_space = self.agent.action_space
        self.observation_space = self.agent.observation_space
        # debug
        self.total_reward = 0
        # An attempt to reward stayin alive
        self.age_reward = (self.agent.happiness(self.ecosystem.terrain) /
                           self.agent.hyperparameters['max_age']
                          )

    def _next_observation(self):
        return self.agent.observation(self.ecosystem.terrain)

    # Helper function to step.
    def _take_action(self, action):
        # Maybe this should be handled by the ecosystem ie the call should be
        # self.ecosystem.update(action, agent)
#         print(type(self))
        self.agent.update(action, self.ecosystem.terrain)
#     terrain update as well
        self.position = self.agent.position.copy()
        return self.position

    # Reward functions
    def _reward(self):
        happiness = self.agent.happiness(self.ecosystem.terrain)
        r = happiness - self.p_happiness
        self.p_happiness = happiness
        # TODO: Clean handling of resource punishment
        return r + self.age_reward - 3 * self.agent.out_of_resources()

    def _is_done(self):
        """Done if maximum number of step exceed or if
        we are outside the region. """ # or if we die?
        return self.current_step > EPISODE_LENGTH or self._is_outside()

    def _is_outside(self): return np.abs(self.position).max() > self.side / 2

    def _is_close(self):
        return distance(self.position, self.objects).min() < self.reward_radius

    # Execute one time step within the environment
    def step(self, action):
        self._take_action(action)
        self.current_step += 1

        reward = self._reward()
        self.total_reward += reward
        obs = self._next_observation()
        done = self.ecosystem.is_done()

        return obs, reward, done, {}

    # Reset the state of the environment to an initial state
    def reset(self):
        print(f"Reset@{self.current_step}, accumulated reward: {self.total_reward:.2f}", end="")
        print(", Interoception levels: ", end="")
        print(*[f'{k}:{v:.2f}' for k,v in self.agent.interoception.items()], sep=', ', end="")
        print(f' happiness: {self.agent.happiness(self.ecosystem.terrain):.2f}')
        self.total_reward = 0

        self.ecosystem.reset()
        self.position = self.agent.position.copy()
        self.current_step = 0
        self.p_happiness = self.agent.happiness(self.ecosystem.terrain)
#         self.consumed = []
        return self._next_observation()

    # Render the environment to the screen
    def render(self, trace, mode='human', close=False):
        # @TODO This should be handled properly
        side = self.ecosystem.terrain.space[0,1] - self.ecosystem.terrain.space[0,0]
        # @TODO Should support any number of object types not just one.
        objects = next(iter(self.ecosystem.terrain.objects.values()))
        motion_diagram(objects, trace, side)

